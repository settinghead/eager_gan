{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qe52EgjARmxs"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-F5N0_7uvTVa"
   },
   "source": [
    "### Import the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GWgwc8aDRmxv"
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) =  tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "print(\"Train data shape:\", X_train.shape)\n",
    "print(\"Train labels shape:\", y_train.shape)\n",
    "\n",
    "print(\"Test data shape:\", X_test.shape)\n",
    "print(\"Test labels shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XcLKypPdvTVf"
   },
   "source": [
    "### Examine the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dD_5UkOVRmx1"
   },
   "outputs": [],
   "source": [
    "labels = range(10)\n",
    "n_classes = len(labels)\n",
    "print(\"Number of classes:\", n_classes)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(18,8))\n",
    "\n",
    "n_columns = 12\n",
    "n_rows = 4\n",
    "\n",
    "for i in range(1,n_columns*n_rows+1):\n",
    "    fig.add_subplot(n_rows, n_columns, i)\n",
    "    plt.imshow(X_train[i])\n",
    "    plt.title(labels[y_train[i]])\n",
    "    # Turn off tick labels\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AwkU8mhKRmx5"
   },
   "outputs": [],
   "source": [
    "W = H = 28\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "\n",
    "def normalizer(image, label):\n",
    "    image = 2 * (tf.to_float(image)) / 255 - 1.\n",
    "    return image, label\n",
    "\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_dataset = train_dataset.map(normalizer).shuffle(1000).batch(BATCH_SIZE)\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UF6Ncpp2TjmF"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, ReLU, PReLU, Activation, Flatten,\n",
    "    Dropout, UpSampling2D, Convolution2D, LeakyReLU,\n",
    "    Input, BatchNormalization, Reshape,\n",
    ")\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "Z_SIZE = 64\n",
    "ALPHA = 0.2\n",
    "BETA1 = 0.5\n",
    "SMOOTH = 0.1\n",
    "\n",
    "def get_generator_model():\n",
    "    n_features = 128\n",
    "    return Sequential([\n",
    "        Dense(n_features, input_shape=[Z_SIZE]),\n",
    "        BatchNormalization(),\n",
    "        LeakyReLU(ALPHA),\n",
    "        Dense(n_features * 2),\n",
    "        BatchNormalization(),\n",
    "        LeakyReLU(ALPHA),\n",
    "        Dense(n_features * 4),\n",
    "        BatchNormalization(),\n",
    "        LeakyReLU(ALPHA),\n",
    "        Dense(W * H),\n",
    "        Activation('tanh'),\n",
    "        Reshape((W, H))\n",
    "    ])\n",
    "\n",
    "def get_discriminator_model():\n",
    "    n_features = 128\n",
    "    return Sequential([\n",
    "        Flatten(input_shape=(W, H)),\n",
    "        Dense(n_features * 4),\n",
    "        LeakyReLU(ALPHA),\n",
    "        Dropout(0.3),\n",
    "        Dense(n_features * 2),\n",
    "        LeakyReLU(ALPHA),\n",
    "        Dropout(0.3),\n",
    "        Dense(n_features),\n",
    "        LeakyReLU(ALPHA),\n",
    "        Dense(1),\n",
    "        Activation('sigmoid')\n",
    "    ])\n",
    "\n",
    "print(\"Generator: \"); get_generator_model().summary()\n",
    "print(\"\\nDiscriminator: \"); get_discriminator_model().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def denormalizer(img): \n",
    "    return ((img + 1)*255 / 2).astype(np.uint8)\n",
    "\n",
    "def display_images(dataset, figsize=(6,6)):\n",
    "    fig, axes = plt.subplots(figsize[0], figsize[1], sharex=True, sharey=True, figsize=figsize,)\n",
    "    for ii, ax in enumerate(axes.flatten()):\n",
    "        img = dataset[ii,:,:]\n",
    "        img = denormalizer(img) # Scale back to 0-255\n",
    "        ax.imshow(img, aspect='equal')\n",
    "      \n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "from tensorflow.keras.backend import binary_crossentropy\n",
    "def discriminator_loss(outputs_real, outputs_fake):\n",
    "  # for the real image from the training set, we want them to be classified as positives,  \n",
    "  # so we want their labels to be all ones. \n",
    "    loss_real = tf.reduce_mean(\n",
    "        binary_crossentropy(\n",
    "            output=outputs_real, \n",
    "            target=tf.ones_like(outputs_real)\n",
    "       )\n",
    "    )\n",
    "\n",
    "  # for the fake images produced by the generator, we want the discriminator to classify them as false images,\n",
    "  # so we set their labels to be all zeros.\n",
    "    loss_fake = tf.reduce_mean(\n",
    "        tf.keras.backend.binary_crossentropy(\n",
    "            output=outputs_fake, \n",
    "            target=tf.zeros_like(outputs_fake)\n",
    "        )\n",
    "    )\n",
    "    d_loss = tf.stack(loss_real + loss_fake, 0)\n",
    "    return d_loss\n",
    "    \n",
    "def generator_loss(d_logits_fake):\n",
    "    # since the generator wants the discriminator to output 1s for its images, it uses the discriminator logits for the\n",
    "    # fake images and assign labels of 1s to them.\n",
    "    g_loss = tf.reduce_mean(\n",
    "        tf.keras.backend.binary_crossentropy(\n",
    "            output=d_logits_fake, \n",
    "            target=tf.ones_like(d_logits_fake)\n",
    "        )\n",
    "    ) \n",
    "    return g_loss\n",
    "\n",
    "from tensorflow.train import AdamOptimizer\n",
    "import time\n",
    "def train(epochs, gen_net, dis_net):\n",
    "    generator_optimizer = AdamOptimizer(LEARNING_RATE, BETA1)\n",
    "    discriminator_optimizer = AdamOptimizer(LEARNING_RATE, BETA1)\n",
    "\n",
    "    # generate sample noise for evaluation\n",
    "    fake_input_test = tf.random_uniform(shape=(6 * 6, Z_SIZE),\n",
    "                                     minval=-1.0, maxval=1.0, dtype=tf.float32)\n",
    "    \n",
    "    init_untrained_samples = gen_net(fake_input_test, training=False)\n",
    "    display_images(init_untrained_samples.numpy())\n",
    "        \n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        for batch_real_images, batch_real_labels in train_dataset:\n",
    "            fake_input = tf.random_uniform(\n",
    "                shape=(BATCH_SIZE, Z_SIZE), minval=-1.0, maxval=1.0, dtype=tf.float32\n",
    "            )\n",
    "            with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "                # run the generator with the random noise batch\n",
    "                g_images = gen_net(fake_input, training=True)\n",
    "\n",
    "                # run the discriminator with real and generated images as inputs \n",
    "                d_logits = dis_net(tf.concat([batch_real_images, g_images], axis=0), training=True)\n",
    "\n",
    "                d_logits_real = d_logits[:BATCH_SIZE]\n",
    "                d_logits_fake = d_logits[BATCH_SIZE:]\n",
    "\n",
    "                # compute the generator loss\n",
    "                gen_loss = generator_loss(d_logits_fake)\n",
    "\n",
    "                # compute the discriminator loss\n",
    "                dis_loss = discriminator_loss(d_logits_real, d_logits_fake)\n",
    "\n",
    "            discriminator_grads = tape.gradient(dis_loss, dis_net.variables)\n",
    "            generator_grads = tape.gradient(gen_loss, gen_net.variables)\n",
    "\n",
    "            discriminator_optimizer.apply_gradients(zip(discriminator_grads, dis_net.variables))\n",
    "            generator_optimizer.apply_gradients(zip(generator_grads, gen_net.variables))\n",
    "\n",
    "        \n",
    "        generated_samples = gen_net(fake_input_test, training=False)\n",
    "#         from IPython.display import clear_output\n",
    "#         clear_output()\n",
    "        display_images(generated_samples.numpy())\n",
    "        print('epoch: ',  epoch,\n",
    "              ', g_loss: ', gen_loss.numpy(), \n",
    "              ', d_loss: ', dis_loss.numpy(), \n",
    "              ', time: ', time.time() - start\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8JniKjWiRmx8",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0002\n",
    "EPOCHS = 150\n",
    "\n",
    "generator_net = get_generator_model()\n",
    "discriminator_net = get_discriminator_model()\n",
    "\n",
    "train(EPOCHS, generator_net, discriminator_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DCGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, ReLU, PReLU, Activation, Flatten, Average,\n",
    "    Dropout, UpSampling2D, Conv2D, LeakyReLU, Conv2DTranspose,\n",
    "    Input, BatchNormalization, Reshape, UpSampling2D, GlobalAveragePooling2D\n",
    ")\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "Z_SIZE = 64\n",
    "ALPHA = 0.2\n",
    "BETA1 = 0.5\n",
    "\n",
    "def get_dcgan_generator_model():\n",
    "    return Sequential([\n",
    "        Dense((W / 4) * (H / 4) * 64, input_shape=[Z_SIZE], use_bias=False),\n",
    "        BatchNormalization(),\n",
    "        LeakyReLU(ALPHA),\n",
    "        Reshape(((W / 4), (H / 4), 64)),\n",
    "        Conv2DTranspose(64, (5, 5), strides=(1, 1), padding='same', use_bias=False),\n",
    "        BatchNormalization(),\n",
    "        LeakyReLU(ALPHA),\n",
    "        Conv2DTranspose(32, (5, 5), strides=(2, 2), padding='same', use_bias=False),\n",
    "        BatchNormalization(),\n",
    "        LeakyReLU(ALPHA),\n",
    "        Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False),\n",
    "        Activation('tanh'),\n",
    "        Reshape((W, H))\n",
    "    ])\n",
    "\n",
    "def get_dcgan_discriminator_model():\n",
    "    return Sequential([\n",
    "        Reshape((W, H, 1), input_shape=(W, H)),\n",
    "        Convolution2D(64, (5, 5), strides=(2, 2), padding='same'),\n",
    "        LeakyReLU(ALPHA),\n",
    "        Dropout(0.3),\n",
    "        Convolution2D(128, (5, 5), strides=(2, 2), padding='same'),\n",
    "        LeakyReLU(ALPHA),\n",
    "        Dropout(0.3),\n",
    "        Flatten(),\n",
    "        Dense(1),\n",
    "        Activation('sigmoid')\n",
    "    ])\n",
    "\n",
    "dcgan_generator_net = get_dcgan_generator_model()\n",
    "dcgan_discriminator_net = get_dcgan_discriminator_model()\n",
    "\n",
    "print(\"DCGAN Generator: \"); dcgan_generator_net.summary()\n",
    "print(\"\\nDCGAN Discriminator: \"); dcgan_discriminator_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0002\n",
    "EPOCHS = 150\n",
    "\n",
    "dcgan_generator_net = get_dcgan_generator_model()\n",
    "dcgan_discriminator_net = get_dcgan_discriminator_model()\n",
    "\n",
    "train(EPOCHS, dcgan_generator_net, dcgan_discriminator_net)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "mnist_basic_gan_eager.ipynb",
   "private_outputs": true,
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
